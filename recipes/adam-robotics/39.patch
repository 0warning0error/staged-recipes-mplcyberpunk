From d3e5a0aeff3e112397ce3641f4d1a63dc9474622 Mon Sep 17 00:00:00 2001
From: giulero <gl.giuseppelerario@gmail.com>
Date: Mon, 8 May 2023 13:57:13 +0200
Subject: [PATCH] Fix float problems

---
 src/adam/pytorch/torch_like.py     | 11 +++++++----
 tests/test_Jax_computations.py     |  3 +++
 tests/test_pytorch_computations.py |  2 ++
 3 files changed, 12 insertions(+), 4 deletions(-)

diff --git a/src/adam/pytorch/torch_like.py b/src/adam/pytorch/torch_like.py
index ca47515..f9e8fe9 100644
--- a/src/adam/pytorch/torch_like.py
+++ b/src/adam/pytorch/torch_like.py
@@ -45,21 +45,24 @@ class TorchLike(ArrayLike):
         Returns:
             TorchLike: transpose of array
         """
-        return TorchLike(self.array.T)
+        if len(self.array.shape) != 1:
+            return TorchLike(self.array.mT)
+        x = self.array
+        return TorchLike(x.permute(*torch.arange(x.ndim - 1, -1, -1)))
 
     def __matmul__(self, other: Union["TorchLike", ntp.ArrayLike]) -> "TorchLike":
         """Overrides @ operator"""
         if type(self) is type(other):
-            return TorchLike(self.array @ other.array)
+            return TorchLike(self.array @ other.array.float())
         else:
-            return TorchLike(self.array @ torch.FloatTensor(other))
+            return TorchLike(self.array @ torch.tensor(other).float())
 
     def __rmatmul__(self, other: Union["TorchLike", ntp.ArrayLike]) -> "TorchLike":
         """Overrides @ operator"""
         if type(self) is type(other):
             return TorchLike(other.array @ self.array)
         else:
-            return TorchLike(torch.FloatTensor(other) @ self.array)
+            return TorchLike(torch.tensor(other).float() @ self.array)
 
     def __mul__(self, other: Union["TorchLike", ntp.ArrayLike]) -> "TorchLike":
         """Overrides * operator"""
diff --git a/tests/test_Jax_computations.py b/tests/test_Jax_computations.py
index 2bb81e0..41b4615 100644
--- a/tests/test_Jax_computations.py
+++ b/tests/test_Jax_computations.py
@@ -9,10 +9,13 @@ import idyntree.swig as idyntree
 import jax.numpy as jnp
 import numpy as np
 import pytest
+from jax import config
 
 from adam.geometry import utils
 from adam.jax import KinDynComputations
 
+config.update("jax_enable_x64", True)
+
 model_path = gym_ignition_models.get_model_file("iCubGazeboV2_5")
 
 joints_name_list = [
diff --git a/tests/test_pytorch_computations.py b/tests/test_pytorch_computations.py
index f400a62..6bead78 100644
--- a/tests/test_pytorch_computations.py
+++ b/tests/test_pytorch_computations.py
@@ -13,6 +13,8 @@ import torch
 from adam.geometry import utils
 from adam.pytorch import KinDynComputations
 
+torch.set_default_dtype(torch.float64)
+
 model_path = gym_ignition_models.get_model_file("iCubGazeboV2_5")
 
 joints_name_list = [
-- 
2.33.0.windows.2

